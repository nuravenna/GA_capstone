{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/cynthiaowens/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/cynthiaowens/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import reddit dataset as csv:\n",
    "\n",
    "#### (from 'Salvaging the Internet Hate Machine: Using the discourse of extremist online subcultures to identify emergent extreme speech'; 2-20-2020; Peeters, Stijn; Hagen, Sal; Das, Partha; University of Amsterdam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note-- the reddit csv I read in, below, is too large to upload to github, so I am including a link to the site from which I found the data. If you have pulled this notebook down from github and plan to run it, please go to this link: https://zenodo.org/record/3676483#.YbBO6PHMLfF and download the reddit-dataset.csv. The below command will not work until you have downloaded that csv into the data folder. \n",
    "\n",
    "# All the other csvs created in the course of these notebooks (with the exception of adjectives.csv, which is only 7KB) have not been included in the data folder because they were also too large to be accepted by github -- they need to be created dynamically by running the cells of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs = pd.read_csv('data/reddit-dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial check of size and content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3618557, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>die hard trump supporter but i dont think thos...</td>\n",
       "      <td>The_Donald</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>His family begged them to intervene.</td>\n",
       "      <td>The_Donald</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>ChapoTrapHouse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>my man sitting next to trumps right is so on p...</td>\n",
       "      <td>ChapoTrapHouse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Those are good reasons. Canada is a great coun...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body       subreddit\n",
       "0  die hard trump supporter but i dont think thos...      The_Donald\n",
       "1               His family begged them to intervene.      The_Donald\n",
       "2                                                NaN  ChapoTrapHouse\n",
       "3  my man sitting next to trumps right is so on p...  ChapoTrapHouse\n",
       "4  Those are good reasons. Canada is a great coun...        politics"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3618557 entries, 0 to 3618556\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Dtype \n",
      "---  ------     ----- \n",
      " 0   body       object\n",
      " 1   subreddit  object\n",
      "dtypes: object(2)\n",
      "memory usage: 55.2+ MB\n"
     ]
    }
   ],
   "source": [
    "subs.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The 'subreddit' column will be the y value-- get a sense of the breakdown in the dataset of the various subreddits each post (row) comes from:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "politics          2379546\n",
       "The_Donald         878217\n",
       "ChapoTrapHouse     348552\n",
       "TheRedPill          12242\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subs['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "politics          0.657595\n",
       "The_Donald        0.242698\n",
       "ChapoTrapHouse    0.096323\n",
       "TheRedPill        0.003383\n",
       "Name: subreddit, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subs['subreddit'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is a very large dataset (more than 3.5 million posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get a sample of the dataframe, download to csv in order to get an idea of the content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = subs.sample(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.to_csv('./data/sample.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete any rows in the dataset with '[removed]' or '[deleted]' tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "body         175131\n",
       "subreddit    175131\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subs.loc[subs['body'] == '[removed]'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "body         23503\n",
       "subreddit    23503\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subs.loc[subs['body'] == '[deleted]'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a sub-dataframe with the above types of rows removed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = subs.loc[subs['body'] != '[removed]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3443426, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = clean.loc[clean['body'] != '[deleted]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3419923, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop all rows with empty/NAN values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "body         118824\n",
       "subreddit         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "body         0\n",
       "subreddit    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3301099, 2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = clean.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After dropping all observations with empty or uninformative content, the cleaned dataset still has more than three million values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete any rows in the dataset that were created by bots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "discard = [\"I am a bot, and this action was performed automatically\"]\n",
    "clean = clean[~clean.body.str.contains('|'.join(discard))]\n",
    "\n",
    "# from:\n",
    "# https://www.statology.org/pandas-drop-rows-that-contain-string/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3196580, 2)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean.shape\n",
    "\n",
    "# lost about 200_000 rows with this, still over three million left in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a new column with the word count of the associated post content for each row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean['word_count'] = clean['body'].map(lambda x: len(x.split(' ')))\n",
    "\n",
    "# from:\n",
    "# Katie Sylvia Breakfast Hour week6 NLP Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>die hard trump supporter but i dont think thos...</td>\n",
       "      <td>The_Donald</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>His family begged them to intervene.</td>\n",
       "      <td>The_Donald</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my man sitting next to trumps right is so on p...</td>\n",
       "      <td>ChapoTrapHouse</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Those are good reasons. Canada is a great coun...</td>\n",
       "      <td>politics</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Oy! The Father of Lies title is taken.\\nour lo...</td>\n",
       "      <td>politics</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body       subreddit  \\\n",
       "0  die hard trump supporter but i dont think thos...      The_Donald   \n",
       "1               His family begged them to intervene.      The_Donald   \n",
       "2  my man sitting next to trumps right is so on p...  ChapoTrapHouse   \n",
       "3  Those are good reasons. Canada is a great coun...        politics   \n",
       "4  Oy! The Father of Lies title is taken.\\nour lo...        politics   \n",
       "\n",
       "   word_count  \n",
       "0          13  \n",
       "1           6  \n",
       "2          11  \n",
       "3           9  \n",
       "4          22  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create another sub-dataframe from the cleaned dataset with just the 'ChapoTrapHouse', 'The_Donald' and 'TheRedPill' subreddits, as these are the ones most likely with extremist language. (The 'politics' subreddit likely has extreme language as well but is interspersed with normal language, and after looking it over I would not classify it as an extremist subreddit.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "extrem = clean[clean['subreddit'] != 'politics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1050497, 3)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extrem.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "The_Donald        720476\n",
       "ChapoTrapHouse    319872\n",
       "TheRedPill         10149\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extrem['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "extrem = extrem.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a sub-dataframe from the cleaned dataset with just the 'politics' subreddit content, potentially to be combined into the model's X as a non-extremist category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_extrem = clean[clean['subreddit'] == 'politics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2146083, 3)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_extrem.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_extrem = non_extrem.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create functions to pre-process the content of the 'body' column-- the actual subreddit posts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to tokenize, remove stop words, punctuation and special chars-\n",
    "\n",
    "def nlp_tokenize(content):\n",
    "    # instantiate tokenizer and stemmer-\n",
    "    p_stem = PorterStemmer()\n",
    "    my_tokenizer = RegexpTokenizer(\"[\\w']+|\\$[\\d\\.]+\") # Katie Sylvia Breakfast Hour week6 NLP Practice\n",
    "    \n",
    "    # tokenize words and make lower case-\n",
    "    words = my_tokenizer.tokenize(content.lower())\n",
    "\n",
    "    # remove stop words-\n",
    "    sw_list = stopwords.words('english')\n",
    "    non_stop = [word for word in words if word not in sw_list] \n",
    "    \n",
    "    return ' '.join(non_stop)\n",
    "    \n",
    "# from:\n",
    "# Katie Sylvia Breakfast Hour week6 NLP Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to stem the words in addition to all the other processing, above-\n",
    "\n",
    "def nlp_stem(content):\n",
    "    # instantiate tokenizer and stemmer-\n",
    "    p_stem = PorterStemmer()\n",
    "    my_tokenizer = RegexpTokenizer(\"[\\w']+|\\$[\\d\\.]+\") # Katie Sylvia Breakfast Hour week6 NLP Practice\n",
    "    \n",
    "    # tokenize words and make lower case-\n",
    "    words = my_tokenizer.tokenize(content.lower())\n",
    "\n",
    "    # remove stop words-\n",
    "    sw_list = stopwords.words('english')\n",
    "    non_stop = [word for word in words if word not in sw_list] \n",
    "    \n",
    "    # stem-\n",
    "    stemmed_list = [p_stem.stem(word) for word in non_stop]\n",
    "\n",
    "    # stitch the list back into a string and return-\n",
    "    return ' '.join(stemmed_list)\n",
    "    \n",
    "# from:\n",
    "# Katie Sylvia Breakfast Hour week6 NLP Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run extrem df thru nlp preprocessing function, creating new tokenized column-\n",
    "\n",
    "extrem['tokenized'] = extrem['body'].map(nlp_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run extrem df thru the stemming function, creating new clean_content column-\n",
    "\n",
    "extrem['clean_content'] = extrem['body'].map(nlp_stem)\n",
    "\n",
    "# these both took a really long time so I'm going to hold off on running it across the entire non_extrem dataset for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>word_count</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>clean_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>die hard trump supporter but i dont think thos...</td>\n",
       "      <td>The_Donald</td>\n",
       "      <td>13</td>\n",
       "      <td>die hard trump supporter dont think videos amo...</td>\n",
       "      <td>die hard trump support dont think video amount...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>His family begged them to intervene.</td>\n",
       "      <td>The_Donald</td>\n",
       "      <td>6</td>\n",
       "      <td>family begged intervene</td>\n",
       "      <td>famili beg interven</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my man sitting next to trumps right is so on p...</td>\n",
       "      <td>ChapoTrapHouse</td>\n",
       "      <td>11</td>\n",
       "      <td>man sitting next trumps right point</td>\n",
       "      <td>man sit next trump right point</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What a bitch</td>\n",
       "      <td>ChapoTrapHouse</td>\n",
       "      <td>3</td>\n",
       "      <td>bitch</td>\n",
       "      <td>bitch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We'll have advance notice of them planning to ...</td>\n",
       "      <td>The_Donald</td>\n",
       "      <td>28</td>\n",
       "      <td>we'll advance notice planning attack children ...</td>\n",
       "      <td>we'll advanc notic plan attack children wealth...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body       subreddit  \\\n",
       "0  die hard trump supporter but i dont think thos...      The_Donald   \n",
       "1               His family begged them to intervene.      The_Donald   \n",
       "2  my man sitting next to trumps right is so on p...  ChapoTrapHouse   \n",
       "3                                       What a bitch  ChapoTrapHouse   \n",
       "4  We'll have advance notice of them planning to ...      The_Donald   \n",
       "\n",
       "   word_count                                          tokenized  \\\n",
       "0          13  die hard trump supporter dont think videos amo...   \n",
       "1           6                            family begged intervene   \n",
       "2          11                man sitting next trumps right point   \n",
       "3           3                                              bitch   \n",
       "4          28  we'll advance notice planning attack children ...   \n",
       "\n",
       "                                       clean_content  \n",
       "0  die hard trump support dont think video amount...  \n",
       "1                                famili beg interven  \n",
       "2                     man sit next trump right point  \n",
       "3                                              bitch  \n",
       "4  we'll advanc notic plan attack children wealth...  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extrem.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the 50 observations in the extrem dataset with the highest word counts, check for junk rows that you can remove:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>clean_content</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32554</th>\n",
       "      <td>ChapoTrapHouse</td>\n",
       "      <td>ok boomer ok boomer ok boomer ok boomer ok boo...</td>\n",
       "      <td>8004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1011207</th>\n",
       "      <td>ChapoTrapHouse</td>\n",
       "      <td>john galt speech ayn rand atla shrug twelv yea...</td>\n",
       "      <td>7070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409483</th>\n",
       "      <td>ChapoTrapHouse</td>\n",
       "      <td>ii izw wx ix wmnz zznxmw w w mn zm w nw x w ii...</td>\n",
       "      <td>6643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>903805</th>\n",
       "      <td>ChapoTrapHouse</td>\n",
       "      <td>neoliber exist concept within academ circl nea...</td>\n",
       "      <td>4270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536470</th>\n",
       "      <td>The_Donald</td>\n",
       "      <td>trump' accomplish creat histor econom boom due...</td>\n",
       "      <td>4186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78533</th>\n",
       "      <td>ChapoTrapHouse</td>\n",
       "      <td>1929 cultur lost secretari state henri stimson...</td>\n",
       "      <td>4177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76049</th>\n",
       "      <td>ChapoTrapHouse</td>\n",
       "      <td>metal gear solid militari masculin homoerotica...</td>\n",
       "      <td>3700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1043375</th>\n",
       "      <td>The_Donald</td>\n",
       "      <td>perpetu polit institut address young men' lyce...</td>\n",
       "      <td>3526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700621</th>\n",
       "      <td>TheRedPill</td>\n",
       "      <td>mayb bit like social event parti gather wed et...</td>\n",
       "      <td>3477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>884341</th>\n",
       "      <td>TheRedPill</td>\n",
       "      <td>decid pull nbsp talk girl attract determin ide...</td>\n",
       "      <td>2923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508045</th>\n",
       "      <td>The_Donald</td>\n",
       "      <td>we'v got lot studi show white peopl react immi...</td>\n",
       "      <td>2722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>819890</th>\n",
       "      <td>The_Donald</td>\n",
       "      <td>sick 20 year gut issu allergi chronic lyme dis...</td>\n",
       "      <td>2668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139373</th>\n",
       "      <td>TheRedPill</td>\n",
       "      <td>hello everyon tl dr nofap benefit real placebo...</td>\n",
       "      <td>2578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>965738</th>\n",
       "      <td>The_Donald</td>\n",
       "      <td>excerpt http www americanpartisan org 2019 10 ...</td>\n",
       "      <td>2409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>633690</th>\n",
       "      <td>ChapoTrapHouse</td>\n",
       "      <td>never post http theanarchistlibrari org librar...</td>\n",
       "      <td>2317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720903</th>\n",
       "      <td>ChapoTrapHouse</td>\n",
       "      <td>ᴬ ˢᵖᵉᶜᵗʳᵉ ᶦˢ ʰᵃᵘⁿᵗᶦⁿᵍ ᴱᵘʳᵒᵖᵉ ᵗʰᵉ ˢᵖᵉᶜᵗʳᵉ ᵒᶠ ᶜᵒ...</td>\n",
       "      <td>2281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198375</th>\n",
       "      <td>ChapoTrapHouse</td>\n",
       "      <td>two factor commod use valu valu substanc valu ...</td>\n",
       "      <td>2268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686379</th>\n",
       "      <td>TheRedPill</td>\n",
       "      <td>tldr post go break exactli quit bad habit chan...</td>\n",
       "      <td>2166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270618</th>\n",
       "      <td>ChapoTrapHouse</td>\n",
       "      <td>mask fuck mask mask fuck mask mask fuck mask m...</td>\n",
       "      <td>2142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879394</th>\n",
       "      <td>ChapoTrapHouse</td>\n",
       "      <td>yeah read right can't stop sex instant ramen n...</td>\n",
       "      <td>2128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256400</th>\n",
       "      <td>ChapoTrapHouse</td>\n",
       "      <td>ok boomer ok boomer ok boomer ok boomer ok boo...</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179602</th>\n",
       "      <td>ChapoTrapHouse</td>\n",
       "      <td>ok boomer ok boomer ok boomer ok boomer ok boo...</td>\n",
       "      <td>1998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>891236</th>\n",
       "      <td>ChapoTrapHouse</td>\n",
       "      <td>ok boomer ok boomer ok boomer ok boomer ok boo...</td>\n",
       "      <td>1998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700165</th>\n",
       "      <td>ChapoTrapHouse</td>\n",
       "      <td>well ya better get back locker make real probl...</td>\n",
       "      <td>1983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346199</th>\n",
       "      <td>The_Donald</td>\n",
       "      <td>environmentalist caus hijack croni capitalist ...</td>\n",
       "      <td>1919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>668479</th>\n",
       "      <td>ChapoTrapHouse</td>\n",
       "      <td>second entri horror social aim fix haunt speci...</td>\n",
       "      <td>1901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370767</th>\n",
       "      <td>ChapoTrapHouse</td>\n",
       "      <td>got done read uninhabit earth http www crisrie...</td>\n",
       "      <td>1894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324659</th>\n",
       "      <td>ChapoTrapHouse</td>\n",
       "      <td>got done read uninhabit earth http www crisrie...</td>\n",
       "      <td>1892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289727</th>\n",
       "      <td>The_Donald</td>\n",
       "      <td>_____ _____ _____ _____ _______ _____ __ __ __...</td>\n",
       "      <td>1885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146090</th>\n",
       "      <td>TheRedPill</td>\n",
       "      <td>major idea promot red pill fall three main cat...</td>\n",
       "      <td>1872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>591016</th>\n",
       "      <td>TheRedPill</td>\n",
       "      <td>understand appli trp women depend intend outco...</td>\n",
       "      <td>1866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115168</th>\n",
       "      <td>ChapoTrapHouse</td>\n",
       "      <td>1 3 new yorker paywal quot 3 3 rob delaney wri...</td>\n",
       "      <td>1768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309248</th>\n",
       "      <td>ChapoTrapHouse</td>\n",
       "      <td>fuck cnn fuck cnn fuck cnn fuck cnn fuck cnn f...</td>\n",
       "      <td>1736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174873</th>\n",
       "      <td>TheRedPill</td>\n",
       "      <td>i'v cheat like past first girlfriend know neur...</td>\n",
       "      <td>1702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45335</th>\n",
       "      <td>TheRedPill</td>\n",
       "      <td>may semi long post tri keep short possibl see ...</td>\n",
       "      <td>1698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353648</th>\n",
       "      <td>ChapoTrapHouse</td>\n",
       "      <td>praxi praxi praxi praxi praxi praxi praxi prax...</td>\n",
       "      <td>1696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58033</th>\n",
       "      <td>TheRedPill</td>\n",
       "      <td>miss call bullshit whole thing ran across post...</td>\n",
       "      <td>1680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>835217</th>\n",
       "      <td>TheRedPill</td>\n",
       "      <td>ran across post today unabl cross post goe i'm...</td>\n",
       "      <td>1677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>835654</th>\n",
       "      <td>TheRedPill</td>\n",
       "      <td>ran across post today cross post goe i'm use t...</td>\n",
       "      <td>1676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315664</th>\n",
       "      <td>The_Donald</td>\n",
       "      <td>mind help search engin winni pooh winni pooh w...</td>\n",
       "      <td>1668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287989</th>\n",
       "      <td>The_Donald</td>\n",
       "      <td>part 8 fall hunter went big sur california att...</td>\n",
       "      <td>1666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157993</th>\n",
       "      <td>ChapoTrapHouse</td>\n",
       "      <td>mask fuck mask mask yeah fuck mask mask fuck m...</td>\n",
       "      <td>1664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253396</th>\n",
       "      <td>ChapoTrapHouse</td>\n",
       "      <td>thank repli i'm debat club lectur philosophi g...</td>\n",
       "      <td>1648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977917</th>\n",
       "      <td>The_Donald</td>\n",
       "      <td>squalor continu pit residenti hotel paul pelos...</td>\n",
       "      <td>1630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56415</th>\n",
       "      <td>ChapoTrapHouse</td>\n",
       "      <td>well sub seem much dedic harri potter discuss ...</td>\n",
       "      <td>1628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559172</th>\n",
       "      <td>ChapoTrapHouse</td>\n",
       "      <td>follow work help translat origin publish rabko...</td>\n",
       "      <td>1627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490440</th>\n",
       "      <td>The_Donald</td>\n",
       "      <td>right wake bia get popular new media format st...</td>\n",
       "      <td>1622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89879</th>\n",
       "      <td>TheRedPill</td>\n",
       "      <td>1 gradual displac western nativ islam becom ma...</td>\n",
       "      <td>1621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238983</th>\n",
       "      <td>TheRedPill</td>\n",
       "      <td>reddit go way digg com complet evapor spirit i...</td>\n",
       "      <td>1617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523086</th>\n",
       "      <td>ChapoTrapHouse</td>\n",
       "      <td>copi past googl translat egon krenz new german...</td>\n",
       "      <td>1614</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              subreddit                                      clean_content  \\\n",
       "32554    ChapoTrapHouse  ok boomer ok boomer ok boomer ok boomer ok boo...   \n",
       "1011207  ChapoTrapHouse  john galt speech ayn rand atla shrug twelv yea...   \n",
       "409483   ChapoTrapHouse  ii izw wx ix wmnz zznxmw w w mn zm w nw x w ii...   \n",
       "903805   ChapoTrapHouse  neoliber exist concept within academ circl nea...   \n",
       "536470       The_Donald  trump' accomplish creat histor econom boom due...   \n",
       "78533    ChapoTrapHouse  1929 cultur lost secretari state henri stimson...   \n",
       "76049    ChapoTrapHouse  metal gear solid militari masculin homoerotica...   \n",
       "1043375      The_Donald  perpetu polit institut address young men' lyce...   \n",
       "700621       TheRedPill  mayb bit like social event parti gather wed et...   \n",
       "884341       TheRedPill  decid pull nbsp talk girl attract determin ide...   \n",
       "508045       The_Donald  we'v got lot studi show white peopl react immi...   \n",
       "819890       The_Donald  sick 20 year gut issu allergi chronic lyme dis...   \n",
       "139373       TheRedPill  hello everyon tl dr nofap benefit real placebo...   \n",
       "965738       The_Donald  excerpt http www americanpartisan org 2019 10 ...   \n",
       "633690   ChapoTrapHouse  never post http theanarchistlibrari org librar...   \n",
       "720903   ChapoTrapHouse  ᴬ ˢᵖᵉᶜᵗʳᵉ ᶦˢ ʰᵃᵘⁿᵗᶦⁿᵍ ᴱᵘʳᵒᵖᵉ ᵗʰᵉ ˢᵖᵉᶜᵗʳᵉ ᵒᶠ ᶜᵒ...   \n",
       "198375   ChapoTrapHouse  two factor commod use valu valu substanc valu ...   \n",
       "686379       TheRedPill  tldr post go break exactli quit bad habit chan...   \n",
       "270618   ChapoTrapHouse  mask fuck mask mask fuck mask mask fuck mask m...   \n",
       "879394   ChapoTrapHouse  yeah read right can't stop sex instant ramen n...   \n",
       "256400   ChapoTrapHouse  ok boomer ok boomer ok boomer ok boomer ok boo...   \n",
       "179602   ChapoTrapHouse  ok boomer ok boomer ok boomer ok boomer ok boo...   \n",
       "891236   ChapoTrapHouse  ok boomer ok boomer ok boomer ok boomer ok boo...   \n",
       "700165   ChapoTrapHouse  well ya better get back locker make real probl...   \n",
       "346199       The_Donald  environmentalist caus hijack croni capitalist ...   \n",
       "668479   ChapoTrapHouse  second entri horror social aim fix haunt speci...   \n",
       "370767   ChapoTrapHouse  got done read uninhabit earth http www crisrie...   \n",
       "324659   ChapoTrapHouse  got done read uninhabit earth http www crisrie...   \n",
       "289727       The_Donald  _____ _____ _____ _____ _______ _____ __ __ __...   \n",
       "146090       TheRedPill  major idea promot red pill fall three main cat...   \n",
       "591016       TheRedPill  understand appli trp women depend intend outco...   \n",
       "115168   ChapoTrapHouse  1 3 new yorker paywal quot 3 3 rob delaney wri...   \n",
       "309248   ChapoTrapHouse  fuck cnn fuck cnn fuck cnn fuck cnn fuck cnn f...   \n",
       "174873       TheRedPill  i'v cheat like past first girlfriend know neur...   \n",
       "45335        TheRedPill  may semi long post tri keep short possibl see ...   \n",
       "353648   ChapoTrapHouse  praxi praxi praxi praxi praxi praxi praxi prax...   \n",
       "58033        TheRedPill  miss call bullshit whole thing ran across post...   \n",
       "835217       TheRedPill  ran across post today unabl cross post goe i'm...   \n",
       "835654       TheRedPill  ran across post today cross post goe i'm use t...   \n",
       "315664       The_Donald  mind help search engin winni pooh winni pooh w...   \n",
       "287989       The_Donald  part 8 fall hunter went big sur california att...   \n",
       "157993   ChapoTrapHouse  mask fuck mask mask yeah fuck mask mask fuck m...   \n",
       "253396   ChapoTrapHouse  thank repli i'm debat club lectur philosophi g...   \n",
       "977917       The_Donald  squalor continu pit residenti hotel paul pelos...   \n",
       "56415    ChapoTrapHouse  well sub seem much dedic harri potter discuss ...   \n",
       "559172   ChapoTrapHouse  follow work help translat origin publish rabko...   \n",
       "490440       The_Donald  right wake bia get popular new media format st...   \n",
       "89879        TheRedPill  1 gradual displac western nativ islam becom ma...   \n",
       "238983       TheRedPill  reddit go way digg com complet evapor spirit i...   \n",
       "523086   ChapoTrapHouse  copi past googl translat egon krenz new german...   \n",
       "\n",
       "         word_count  \n",
       "32554          8004  \n",
       "1011207        7070  \n",
       "409483         6643  \n",
       "903805         4270  \n",
       "536470         4186  \n",
       "78533          4177  \n",
       "76049          3700  \n",
       "1043375        3526  \n",
       "700621         3477  \n",
       "884341         2923  \n",
       "508045         2722  \n",
       "819890         2668  \n",
       "139373         2578  \n",
       "965738         2409  \n",
       "633690         2317  \n",
       "720903         2281  \n",
       "198375         2268  \n",
       "686379         2166  \n",
       "270618         2142  \n",
       "879394         2128  \n",
       "256400         1999  \n",
       "179602         1998  \n",
       "891236         1998  \n",
       "700165         1983  \n",
       "346199         1919  \n",
       "668479         1901  \n",
       "370767         1894  \n",
       "324659         1892  \n",
       "289727         1885  \n",
       "146090         1872  \n",
       "591016         1866  \n",
       "115168         1768  \n",
       "309248         1736  \n",
       "174873         1702  \n",
       "45335          1698  \n",
       "353648         1696  \n",
       "58033          1680  \n",
       "835217         1677  \n",
       "835654         1676  \n",
       "315664         1668  \n",
       "287989         1666  \n",
       "157993         1664  \n",
       "253396         1648  \n",
       "977917         1630  \n",
       "56415          1628  \n",
       "559172         1627  \n",
       "490440         1622  \n",
       "89879          1621  \n",
       "238983         1617  \n",
       "523086         1614  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extrem.sort_values(['word_count'], ascending=False)[['subreddit', 'clean_content', 'word_count']].head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete any junk rows with large outlying word counts-\n",
    "\n",
    "rows = extrem.index[[32554, 409483, 256400, 179602, 891236, 289727]]\n",
    "\n",
    "extrem = extrem.drop(rows)\n",
    "\n",
    "# from:\n",
    "# https://www.kite.com/python/answers/how-to-drop-a-list-of-rows-from-a-pandas-dataframe-by-index-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "extrem = extrem.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1050491, 6)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extrem.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do the same for the non_extrem dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>body</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1938809</th>\n",
       "      <td>politics</td>\n",
       "      <td>RE: polito’s rightward beltway tilt\\nI hear yo...</td>\n",
       "      <td>1817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1516401</th>\n",
       "      <td>politics</td>\n",
       "      <td>,\\nThailand is no longer a country of rice far...</td>\n",
       "      <td>1707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1721010</th>\n",
       "      <td>politics</td>\n",
       "      <td>&gt;There is not a very large financial barrier t...</td>\n",
       "      <td>1651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1798499</th>\n",
       "      <td>politics</td>\n",
       "      <td>**Trump's record on the military, veterans, an...</td>\n",
       "      <td>1644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464098</th>\n",
       "      <td>politics</td>\n",
       "      <td>&gt; Who gets to design the protocol and platform...</td>\n",
       "      <td>1643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1912624</th>\n",
       "      <td>politics</td>\n",
       "      <td>**Trump's record on the military, veterans, an...</td>\n",
       "      <td>1642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>794030</th>\n",
       "      <td>politics</td>\n",
       "      <td>https://www.nytimes.com/2000/08/09/nyregion/br...</td>\n",
       "      <td>1637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1524827</th>\n",
       "      <td>politics</td>\n",
       "      <td>Feb. 22  2017: \\nProPublica’s Raymond Bonner r...</td>\n",
       "      <td>1635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2092632</th>\n",
       "      <td>politics</td>\n",
       "      <td>For when they talk about how civies would get ...</td>\n",
       "      <td>1631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1672685</th>\n",
       "      <td>politics</td>\n",
       "      <td>1). They seize the lands of sovereign nations ...</td>\n",
       "      <td>1620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119982</th>\n",
       "      <td>politics</td>\n",
       "      <td>You don't need to be the first to still be wro...</td>\n",
       "      <td>1616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>880823</th>\n",
       "      <td>politics</td>\n",
       "      <td>&gt;The reality is Americans elected Trump and no...</td>\n",
       "      <td>1605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1658102</th>\n",
       "      <td>politics</td>\n",
       "      <td>It’s amazing how wrong they are on it.\\nThe pu...</td>\n",
       "      <td>1605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1214257</th>\n",
       "      <td>politics</td>\n",
       "      <td>The frequency and consistency is such a huge p...</td>\n",
       "      <td>1597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986715</th>\n",
       "      <td>politics</td>\n",
       "      <td>It's always nice when you encounter civil deba...</td>\n",
       "      <td>1591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2050232</th>\n",
       "      <td>politics</td>\n",
       "      <td>January 23, 1987: PKK attack a wedding party, ...</td>\n",
       "      <td>1589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695400</th>\n",
       "      <td>politics</td>\n",
       "      <td>For someone with such a high household income ...</td>\n",
       "      <td>1588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1049816</th>\n",
       "      <td>politics</td>\n",
       "      <td>Quasi-legal doesn't even cover this insanity. ...</td>\n",
       "      <td>1586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1151063</th>\n",
       "      <td>politics</td>\n",
       "      <td>For those of you that couldn’t read because of...</td>\n",
       "      <td>1579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1507595</th>\n",
       "      <td>politics</td>\n",
       "      <td>Yes, downvoters, the Soviet Union was capitali...</td>\n",
       "      <td>1574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2101241</th>\n",
       "      <td>politics</td>\n",
       "      <td>Copying from my post in the megathread, becaus...</td>\n",
       "      <td>1573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521856</th>\n",
       "      <td>politics</td>\n",
       "      <td>May want to look at HUMAN History, kind of why...</td>\n",
       "      <td>1565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1141705</th>\n",
       "      <td>politics</td>\n",
       "      <td>&gt; WASHINGTON — President Trump vowed on Monday...</td>\n",
       "      <td>1560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530000</th>\n",
       "      <td>politics</td>\n",
       "      <td>Hey Preech, would you consider adding this lin...</td>\n",
       "      <td>1548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227192</th>\n",
       "      <td>politics</td>\n",
       "      <td>&gt;You're entirely wrong. First, a silencer make...</td>\n",
       "      <td>1546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614090</th>\n",
       "      <td>politics</td>\n",
       "      <td>&gt; Other countries do have weapons, but stats d...</td>\n",
       "      <td>1542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>657458</th>\n",
       "      <td>politics</td>\n",
       "      <td>Ok I think people here are missing just how co...</td>\n",
       "      <td>1538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8834</th>\n",
       "      <td>politics</td>\n",
       "      <td>1) Why don’t Trump supporters turn against Tru...</td>\n",
       "      <td>1536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77901</th>\n",
       "      <td>politics</td>\n",
       "      <td>**May 11, 2017**: Authorities arrested Steven ...</td>\n",
       "      <td>1535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2102703</th>\n",
       "      <td>politics</td>\n",
       "      <td>How many terrorist attacks has \"Antifa\" commit...</td>\n",
       "      <td>1530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1652692</th>\n",
       "      <td>politics</td>\n",
       "      <td>This. Did you see Michael Moore’s post that sp...</td>\n",
       "      <td>1527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87051</th>\n",
       "      <td>politics</td>\n",
       "      <td>&gt; The country is entering a new and precarious...</td>\n",
       "      <td>1521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2113688</th>\n",
       "      <td>politics</td>\n",
       "      <td>In one of his first calls with a head of state...</td>\n",
       "      <td>1520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2101754</th>\n",
       "      <td>politics</td>\n",
       "      <td>\"The left isn't violent\"  \\nOK:  \\n**Aug. 19, ...</td>\n",
       "      <td>1515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1778091</th>\n",
       "      <td>politics</td>\n",
       "      <td>If you're going to go to all that trouble, you...</td>\n",
       "      <td>1509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1678543</th>\n",
       "      <td>politics</td>\n",
       "      <td>Barbara Jordan, former Representative from Tex...</td>\n",
       "      <td>1501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>657605</th>\n",
       "      <td>politics</td>\n",
       "      <td>&gt; I think the big mistake that Sanders &amp; Warre...</td>\n",
       "      <td>1498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455966</th>\n",
       "      <td>politics</td>\n",
       "      <td>WASHINGTON — Senator Elizabeth Warren has buil...</td>\n",
       "      <td>1490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1963410</th>\n",
       "      <td>politics</td>\n",
       "      <td>https://m.huffpost.com/us/entry/us_5d9b9880e4b...</td>\n",
       "      <td>1490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2097580</th>\n",
       "      <td>politics</td>\n",
       "      <td>&gt;Secretary of State Mike Pompeo admitted Wedne...</td>\n",
       "      <td>1486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1928762</th>\n",
       "      <td>politics</td>\n",
       "      <td>https://thehill.com/opinion/white-house/441892...</td>\n",
       "      <td>1483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1325342</th>\n",
       "      <td>politics</td>\n",
       "      <td>\"A free people ought not only to be armed, but...</td>\n",
       "      <td>1481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1234903</th>\n",
       "      <td>politics</td>\n",
       "      <td>\"A free people ought not only to be armed, but...</td>\n",
       "      <td>1481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470452</th>\n",
       "      <td>politics</td>\n",
       "      <td>\"A free people ought not only to be armed, but...</td>\n",
       "      <td>1481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>606217</th>\n",
       "      <td>politics</td>\n",
       "      <td>&gt; Complain on Reddit.\\nAnd that sums up the lo...</td>\n",
       "      <td>1475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1467131</th>\n",
       "      <td>politics</td>\n",
       "      <td>[Here](https://outline.com/3thDEh) is the link...</td>\n",
       "      <td>1470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239884</th>\n",
       "      <td>politics</td>\n",
       "      <td>&gt;The measures I outlined were Pete’s, by the w...</td>\n",
       "      <td>1469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890624</th>\n",
       "      <td>politics</td>\n",
       "      <td>&gt;In the car is the entire traveling staff, whi...</td>\n",
       "      <td>1467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153412</th>\n",
       "      <td>politics</td>\n",
       "      <td>&gt;In the car is the entire traveling staff, whi...</td>\n",
       "      <td>1467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1218232</th>\n",
       "      <td>politics</td>\n",
       "      <td>WSJ just now: https://www.wsj.com/articles/tru...</td>\n",
       "      <td>1466</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        subreddit                                               body  \\\n",
       "1938809  politics  RE: polito’s rightward beltway tilt\\nI hear yo...   \n",
       "1516401  politics  ,\\nThailand is no longer a country of rice far...   \n",
       "1721010  politics  >There is not a very large financial barrier t...   \n",
       "1798499  politics  **Trump's record on the military, veterans, an...   \n",
       "464098   politics  > Who gets to design the protocol and platform...   \n",
       "1912624  politics  **Trump's record on the military, veterans, an...   \n",
       "794030   politics  https://www.nytimes.com/2000/08/09/nyregion/br...   \n",
       "1524827  politics  Feb. 22  2017: \\nProPublica’s Raymond Bonner r...   \n",
       "2092632  politics  For when they talk about how civies would get ...   \n",
       "1672685  politics  1). They seize the lands of sovereign nations ...   \n",
       "119982   politics  You don't need to be the first to still be wro...   \n",
       "880823   politics  >The reality is Americans elected Trump and no...   \n",
       "1658102  politics  It’s amazing how wrong they are on it.\\nThe pu...   \n",
       "1214257  politics  The frequency and consistency is such a huge p...   \n",
       "986715   politics  It's always nice when you encounter civil deba...   \n",
       "2050232  politics  January 23, 1987: PKK attack a wedding party, ...   \n",
       "695400   politics  For someone with such a high household income ...   \n",
       "1049816  politics  Quasi-legal doesn't even cover this insanity. ...   \n",
       "1151063  politics  For those of you that couldn’t read because of...   \n",
       "1507595  politics  Yes, downvoters, the Soviet Union was capitali...   \n",
       "2101241  politics  Copying from my post in the megathread, becaus...   \n",
       "521856   politics  May want to look at HUMAN History, kind of why...   \n",
       "1141705  politics  > WASHINGTON — President Trump vowed on Monday...   \n",
       "530000   politics  Hey Preech, would you consider adding this lin...   \n",
       "227192   politics  >You're entirely wrong. First, a silencer make...   \n",
       "614090   politics  > Other countries do have weapons, but stats d...   \n",
       "657458   politics  Ok I think people here are missing just how co...   \n",
       "8834     politics  1) Why don’t Trump supporters turn against Tru...   \n",
       "77901    politics  **May 11, 2017**: Authorities arrested Steven ...   \n",
       "2102703  politics  How many terrorist attacks has \"Antifa\" commit...   \n",
       "1652692  politics  This. Did you see Michael Moore’s post that sp...   \n",
       "87051    politics  > The country is entering a new and precarious...   \n",
       "2113688  politics  In one of his first calls with a head of state...   \n",
       "2101754  politics  \"The left isn't violent\"  \\nOK:  \\n**Aug. 19, ...   \n",
       "1778091  politics  If you're going to go to all that trouble, you...   \n",
       "1678543  politics  Barbara Jordan, former Representative from Tex...   \n",
       "657605   politics  > I think the big mistake that Sanders & Warre...   \n",
       "1455966  politics  WASHINGTON — Senator Elizabeth Warren has buil...   \n",
       "1963410  politics  https://m.huffpost.com/us/entry/us_5d9b9880e4b...   \n",
       "2097580  politics  >Secretary of State Mike Pompeo admitted Wedne...   \n",
       "1928762  politics  https://thehill.com/opinion/white-house/441892...   \n",
       "1325342  politics  \"A free people ought not only to be armed, but...   \n",
       "1234903  politics  \"A free people ought not only to be armed, but...   \n",
       "470452   politics  \"A free people ought not only to be armed, but...   \n",
       "606217   politics  > Complain on Reddit.\\nAnd that sums up the lo...   \n",
       "1467131  politics  [Here](https://outline.com/3thDEh) is the link...   \n",
       "239884   politics  >The measures I outlined were Pete’s, by the w...   \n",
       "890624   politics  >In the car is the entire traveling staff, whi...   \n",
       "153412   politics  >In the car is the entire traveling staff, whi...   \n",
       "1218232  politics  WSJ just now: https://www.wsj.com/articles/tru...   \n",
       "\n",
       "         word_count  \n",
       "1938809        1817  \n",
       "1516401        1707  \n",
       "1721010        1651  \n",
       "1798499        1644  \n",
       "464098         1643  \n",
       "1912624        1642  \n",
       "794030         1637  \n",
       "1524827        1635  \n",
       "2092632        1631  \n",
       "1672685        1620  \n",
       "119982         1616  \n",
       "880823         1605  \n",
       "1658102        1605  \n",
       "1214257        1597  \n",
       "986715         1591  \n",
       "2050232        1589  \n",
       "695400         1588  \n",
       "1049816        1586  \n",
       "1151063        1579  \n",
       "1507595        1574  \n",
       "2101241        1573  \n",
       "521856         1565  \n",
       "1141705        1560  \n",
       "530000         1548  \n",
       "227192         1546  \n",
       "614090         1542  \n",
       "657458         1538  \n",
       "8834           1536  \n",
       "77901          1535  \n",
       "2102703        1530  \n",
       "1652692        1527  \n",
       "87051          1521  \n",
       "2113688        1520  \n",
       "2101754        1515  \n",
       "1778091        1509  \n",
       "1678543        1501  \n",
       "657605         1498  \n",
       "1455966        1490  \n",
       "1963410        1490  \n",
       "2097580        1486  \n",
       "1928762        1483  \n",
       "1325342        1481  \n",
       "1234903        1481  \n",
       "470452         1481  \n",
       "606217         1475  \n",
       "1467131        1470  \n",
       "239884         1469  \n",
       "890624         1467  \n",
       "153412         1467  \n",
       "1218232        1466  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_extrem.sort_values(['word_count'], ascending=False)[['subreddit', 'body', 'word_count']].head(50)\n",
    "\n",
    "# no junk rows to remove in this list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Since this is such a large dataset, create a subset of just 40_000 observations, split evenly between non-extremist and extremist subreddits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "extrem_subset = extrem.sample(20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_extrem_subset = non_extrem.sample(20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the non_extrem_subset through the two NLP cleaning/stemming functions so that its columns match\n",
    "# the extrem subset-\n",
    "\n",
    "non_extrem_subset['tokenized'] = non_extrem_subset['body'].map(nlp_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_extrem_subset['clean_content'] = non_extrem_subset['body'].map(nlp_stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add a column marking whether the row in question is from an extremist subreddit (1 for any from 'ChapoTrapHouse', 'The_Donald' and 'TheRedPill' subreddits, 0 for any from the 'politics' subreddit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "extrem_subset['extreme'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_extrem_subset['extreme'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['body', 'subreddit', 'word_count', 'tokenized', 'clean_content',\n",
       "       'extreme'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_extrem_subset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['body', 'subreddit', 'word_count', 'tokenized', 'clean_content',\n",
       "       'extreme'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extrem_subset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the two subsets into one df-\n",
    "\n",
    "subreddits_full = pd.concat([extrem_subset, non_extrem_subset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 6)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subreddits_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle-\n",
    "\n",
    "subreddits = subreddits_full.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# from:\n",
    "# https://stackoverflow.com/questions/29576430/shuffle-dataframe-rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 6)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subreddits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>word_count</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>clean_content</th>\n",
       "      <th>extreme</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>- Official slogan of Pepsi</td>\n",
       "      <td>politics</td>\n",
       "      <td>5</td>\n",
       "      <td>official slogan pepsi</td>\n",
       "      <td>offici slogan pepsi</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"took\" should be \"freed\"</td>\n",
       "      <td>ChapoTrapHouse</td>\n",
       "      <td>4</td>\n",
       "      <td>took freed</td>\n",
       "      <td>took freed</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Will tribes be completely free speech?</td>\n",
       "      <td>TheRedPill</td>\n",
       "      <td>6</td>\n",
       "      <td>tribes completely free speech</td>\n",
       "      <td>tribe complet free speech</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Not an accident. They have crowned their queen...</td>\n",
       "      <td>The_Donald</td>\n",
       "      <td>25</td>\n",
       "      <td>accident crowned queen already warren others g...</td>\n",
       "      <td>accid crown queen alreadi warren other get pla...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fucking love this man</td>\n",
       "      <td>The_Donald</td>\n",
       "      <td>4</td>\n",
       "      <td>fucking love man</td>\n",
       "      <td>fuck love man</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>We're well past the point of embarassment.\\nAt...</td>\n",
       "      <td>The_Donald</td>\n",
       "      <td>25</td>\n",
       "      <td>we're well past point embarassment point kind ...</td>\n",
       "      <td>we'r well past point embarass point kind perve...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I absolutely cannot wait till the lasers get f...</td>\n",
       "      <td>The_Donald</td>\n",
       "      <td>17</td>\n",
       "      <td>absolutely cannot wait till lasers get focused...</td>\n",
       "      <td>absolut cannot wait till laser get focus warre...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>She offered CTU a contract that would put a nu...</td>\n",
       "      <td>politics</td>\n",
       "      <td>43</td>\n",
       "      <td>offered ctu contract would put nurse social wo...</td>\n",
       "      <td>offer ctu contract would put nurs social worke...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Ha i get it now...it is funny.</td>\n",
       "      <td>The_Donald</td>\n",
       "      <td>7</td>\n",
       "      <td>ha get funny</td>\n",
       "      <td>ha get funni</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>&gt; RICO \\nRepublicans In Cells Only?</td>\n",
       "      <td>politics</td>\n",
       "      <td>6</td>\n",
       "      <td>rico republicans cells</td>\n",
       "      <td>rico republican cell</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body       subreddit  \\\n",
       "0                         - Official slogan of Pepsi        politics   \n",
       "1                           \"took\" should be \"freed\"  ChapoTrapHouse   \n",
       "2             Will tribes be completely free speech?      TheRedPill   \n",
       "3  Not an accident. They have crowned their queen...      The_Donald   \n",
       "4                              Fucking love this man      The_Donald   \n",
       "5  We're well past the point of embarassment.\\nAt...      The_Donald   \n",
       "6  I absolutely cannot wait till the lasers get f...      The_Donald   \n",
       "7  She offered CTU a contract that would put a nu...        politics   \n",
       "8                     Ha i get it now...it is funny.      The_Donald   \n",
       "9                > RICO \\nRepublicans In Cells Only?        politics   \n",
       "\n",
       "   word_count                                          tokenized  \\\n",
       "0           5                              official slogan pepsi   \n",
       "1           4                                         took freed   \n",
       "2           6                      tribes completely free speech   \n",
       "3          25  accident crowned queen already warren others g...   \n",
       "4           4                                   fucking love man   \n",
       "5          25  we're well past point embarassment point kind ...   \n",
       "6          17  absolutely cannot wait till lasers get focused...   \n",
       "7          43  offered ctu contract would put nurse social wo...   \n",
       "8           7                                       ha get funny   \n",
       "9           6                             rico republicans cells   \n",
       "\n",
       "                                       clean_content  extreme  \n",
       "0                                offici slogan pepsi        0  \n",
       "1                                         took freed        1  \n",
       "2                          tribe complet free speech        1  \n",
       "3  accid crown queen alreadi warren other get pla...        1  \n",
       "4                                      fuck love man        1  \n",
       "5  we'r well past point embarass point kind perve...        1  \n",
       "6  absolut cannot wait till laser get focus warre...        1  \n",
       "7  offer ctu contract would put nurs social worke...        0  \n",
       "8                                       ha get funni        1  \n",
       "9                               rico republican cell        0  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subreddits.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the datasets as csvs for further processing in subsequent notebooks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits.to_csv('./data/subreddits.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "extrem.to_csv('./data/extreme.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non_extrem.to_csv('./data/nonextreme.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean.to_csv('./data/reddit_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
